{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from nilearn import image\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "sys.path.append('..')\n",
    "from utils.data import Subject, load_participant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/ubuntu/data/learning-habits'\n",
    "bids_dir = \"/home/ubuntu/data/learning-habits/bids_dataset/derivatives/fmriprep-24.0.1\"\n",
    "\n",
    "all_sub_ids = load_participant_list(base_dir, file_name='modeling_participants.tsv')\n",
    "\n",
    "sub_ids = all_sub_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 67\n"
     ]
    }
   ],
   "source": [
    "print('Number of subjects:', len(sub_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/repos/learning-habits-analysis/notebooks/../utils/data.py:211: UserWarning: Last 1 trial(s) of block had no response, filling with 0\n",
      "/home/ubuntu/repos/learning-habits-analysis/notebooks/../utils/data.py:211: UserWarning: Last 1 trial(s) of block had no response, filling with 0\n"
     ]
    }
   ],
   "source": [
    "subjects = [Subject(base_dir, sub_id, include_modeling=True, include_imaging=True, bids_dir=bids_dir) for sub_id in sub_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading fmriprep confounds directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = subjects[0].runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfds = subjects[0].get_confounds_path(runs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "for sub in subjects:\n",
    "    conf_df = pd.read_csv(sub.get_confounds_path(runs[0]), sep='\\t').reset_index(drop=True)\n",
    "    conf_df.insert(0, 'sub_id', sub.sub_id)  # put sub_id as first column\n",
    "    parts.append(conf_df)\n",
    "\n",
    "learning1 = pd.concat(parts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "for sub in subjects:\n",
    "    conf_df = pd.read_csv(sub.get_confounds_path(runs[1]), sep='\\t').reset_index(drop=True)\n",
    "    conf_df.insert(0, 'sub_id', sub.sub_id)  # put sub_id as first column\n",
    "    parts.append(conf_df)\n",
    "\n",
    "learning2 = pd.concat(parts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "for sub in subjects:\n",
    "    conf_df = pd.read_csv(sub.get_confounds_path(runs[2]), sep='\\t').reset_index(drop=True)\n",
    "    conf_df.insert(0, 'sub_id', sub.sub_id)  # put sub_id as first column\n",
    "    parts.append(conf_df)\n",
    "\n",
    "test = pd.concat(parts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning1 = learning1.assign(flagged=learning1['framewise_displacement'] > fd_thresh)\n",
    "learning2 = learning2.assign(flagged=learning2['framewise_displacement'] > fd_thresh)\n",
    "test = test.assign(flagged=test['framewise_displacement'] > fd_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_learning = (learning1.sub_id == 'sub-01').sum()\n",
    "N_test = (test.sub_id == 'sub-01').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_learning1 = np.where((learning1.groupby('sub_id').flagged.sum() > 0.2*N_learning).values)[0]\n",
    "excl_learning2 = np.where((learning2.groupby('sub_id').flagged.sum() > 0.2*N_learning).values)[0]\n",
    "excl_test = np.where((test.groupby('sub_id').flagged.sum() > 0.2*N_test).values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excl_learning1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects excluded due to motion:\n",
      "Learning 1\n",
      "['sub-68']\n",
      "Learning 2\n",
      "['sub-44', 'sub-48', 'sub-68']\n",
      "Test\n",
      "['sub-17', 'sub-31', 'sub-48', 'sub-68']\n"
     ]
    }
   ],
   "source": [
    "# Learning 1\n",
    "print('Subjects excluded due to motion:')\n",
    "print('Learning 1')\n",
    "print(['sub-' + sub_ids[i] for i in excl_learning1])\n",
    "print('Learning 2')\n",
    "print(['sub-' + sub_ids[i] for i in excl_learning2])\n",
    "print('Test')\n",
    "print(['sub-' + sub_ids[i] for i in excl_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the loading of confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = subjects[4]\n",
    "conf, mask = sub.load_confounds('learning1', 'basic', False, scrub=0, fd_thresh=.5, std_dvars_thresh=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_dummies = '/mnt/data/learning-habits/spm_format_20250603/sub-01/func/sub-01_ses-1_task-test_run-3_space-MNI152NLin2009cAsym_desc-preproc_bold_motion_with_dummies.txt'\n",
    "with_dummies = pd.read_csv(with_dummies, sep='\\t', header=None)\n",
    "\n",
    "motion = '/mnt/data/learning-habits/spm_format_20250603/sub-01/func/sub-01_ses-1_task-test_run-3_space-MNI152NLin2009cAsym_desc-preproc_bold_motion.txt'\n",
    "motion = pd.read_csv(motion, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code - not very efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reference images to know the shape of the data\n",
    "N_learning = image.load_img(subjects[0].img.get('learning1')).shape[-1]\n",
    "N_test = image.load_img(subjects[0].img.get('test')).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_volumes = np.zeros((len(subjects), 3))\n",
    "all_volumes[:, :2] = N_learning\n",
    "all_volumes[:, 2] = N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_thresholds = [0.3, 0.5, 0.75, 1, 2]\n",
    "std_dvars_thresholds = [1, 1.5, 2, 2.5, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_volumes_for_thresholds(fd_t, sd_t, scrub=0):\n",
    "    \"\"\"\n",
    "    Returns a (len(subjects), 3) array of valid-volume proportions\n",
    "    for the given FD & DVARS thresholds.\n",
    "    \"\"\"\n",
    "    out = np.zeros((len(subjects), 3))\n",
    "    for i, sub in enumerate(subjects):\n",
    "        for j, run in enumerate(sub.runs):\n",
    "            N_block = N_learning if j < 2 else N_test\n",
    "            img_path = sub.img.get(run)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "                _, sample_mask = load_confounds(\n",
    "                    img_path,\n",
    "                    strategy=('motion','high_pass','wm_csf','scrub'),\n",
    "                    scrub=scrub,\n",
    "                    fd_threshold=fd_t,\n",
    "                    std_dvars_threshold=sd_t\n",
    "                )\n",
    "            valid = len(sample_mask) if sample_mask is not None else N_block\n",
    "            out[i, j] = valid / N_block\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = compute_valid_volumes_for_thresholds(0.5, 2.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.2\n",
    "\n",
    "for run in range(3):\n",
    "    subject_ids = [sub_ids[i] for i in range(len(sub_ids)) if n_valid[i, run] < 1-thresh]\n",
    "    print(f'Run {run+1}: {len(subject_ids)} subjects with > {thresh} scrubbed volumes')\n",
    "    print(f'Subject IDs: {subject_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.2\n",
    "\n",
    "for run in range(3):\n",
    "    subject_ids = [sub_ids[i] for i in range(len(sub_ids)) if n_valid[i, run] < 1-thresh]\n",
    "    print(f'Run {run+1}: {len(subject_ids)} subjects with > {thresh} scrubbed volumes')\n",
    "    print(f'Subject IDs: {subject_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in parallel over all (fd, dvars) combos\n",
    "results = Parallel(n_jobs=30)(\n",
    "    delayed(compute_valid_volumes_for_thresholds)(fd_t, sd_t)\n",
    "    for fd_t in fd_thresholds\n",
    "    for sd_t in std_dvars_thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fd = len(fd_thresholds)\n",
    "n_sd = len(std_dvars_thresholds)\n",
    "prop_valid = np.zeros((n_fd, n_sd, len(subjects), 3))\n",
    "\n",
    "k = 0\n",
    "for i_fd, fd_t in enumerate(fd_thresholds):\n",
    "    for j_sd, sd_t in enumerate(std_dvars_thresholds):\n",
    "        prop_valid[i_fd, j_sd] = results[k]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_valid[1,3,0,0]*N_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean_prop_valid = prop_valid.mean(axis=(2, 3))\n",
    "std_prop_valid = prop_valid.std(axis=(2, 3))\n",
    "\n",
    "# Create the annotation text with mean and standard deviation\n",
    "annot = np.array([[\"{:.2f}Â±{:.2f}\".format(mean, std) for mean, std in zip(row_mean, row_std)] \n",
    "                  for row_mean, row_std in zip(mean_prop_valid, std_prop_valid)])\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(9, 7))  # Adjust figure size if needed\n",
    "sns.heatmap(\n",
    "    mean_prop_valid,  # Average over runs and subjects\n",
    "    xticklabels=std_dvars_thresholds, \n",
    "    yticklabels=fd_thresholds, \n",
    "    cmap=\"viridis\", \n",
    "    annot=annot,  # Add mean and std annotations\n",
    "    fmt=\"\",  # No additional formatting needed\n",
    "    cbar_kws={'label': \"Proportion of valid volumes\"}  # Add label to colorbar\n",
    ")\n",
    "\n",
    "# Configure labels\n",
    "plt.xlabel(\"Standard DVARS threshold\")\n",
    "plt.ylabel(\"FD threshold\")\n",
    "plt.title(\"Proportion of Retained Volumes\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the effect of the scrub parameter \n",
    "fd_threshold = 0.5\n",
    "std_dvars_threshold = 2\n",
    "scrub = [1, 2, 3, 4, 5]\n",
    "\n",
    "results = Parallel(n_jobs=5)(\n",
    "    delayed(compute_valid_volumes_for_thresholds)(fd_threshold, std_dvars_threshold, s)\n",
    "    for s in scrub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for varying scrub\n",
    "mean_prop_valid_scrub = np.array([result.mean(axis=(0, 1)) for result in results])\n",
    "std_prop_valid_scrub = np.array([result.std(axis=(0, 1)) for result in results])\n",
    "\n",
    "# Create the bar plot with error bars\n",
    "plt.figure(figsize=(9, 7))  # Adjust figure size if needed\n",
    "plt.bar(scrub, mean_prop_valid_scrub, yerr=std_prop_valid_scrub, capsize=5, color='skyblue', edgecolor='black')\n",
    "plt.ylim((0.7,1))\n",
    "\n",
    "# Configure labels\n",
    "plt.xlabel(\"Scrub parameter\")\n",
    "plt.ylabel(\"Proportion of valid volumes\")\n",
    "plt.title(\"Effect of Scrub on Proportion of Retained Volumes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose specific thresholds\n",
    "chosen_fd_threshold_index = fd_thresholds.index(0.5)\n",
    "chosen_std_threshold_index = std_dvars_thresholds.index(2)\n",
    "\n",
    "# Extract valid volumes for the chosen thresholds\n",
    "valid_volumes = prop_valid[chosen_fd_threshold_index, chosen_std_threshold_index]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].hist(valid_volumes[:, 0], bins=10, label='learning1')\n",
    "axes[0].axvline(valid_volumes[:, 0].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "axes[0].set_title('Learning1')\n",
    "axes[0].set_xlabel('Fraction of valid Volumes')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlim(0.7, 1)\n",
    "\n",
    "axes[1].hist(valid_volumes[:, 1], bins=10, label='learning2')\n",
    "axes[1].axvline(valid_volumes[:, 1].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "axes[1].set_title('Learning2')\n",
    "axes[1].set_xlabel('Fraction of valid Volumes')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlim(0.7, 1)\n",
    "\n",
    "axes[2].hist(valid_volumes[:, 2], bins=10, label='test')\n",
    "axes[2].axvline(valid_volumes[:, 2].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "axes[2].set_title('Test')\n",
    "axes[2].set_xlabel('Fraction of valid Volumes')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xlim(0.7, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusion threshold\n",
    "max_scrub = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(3):\n",
    "    subject_ids = [sub_ids[i] for i in range(len(sub_ids)) if valid_volumes[i, run] < 1-max_scrub]\n",
    "    print(f'Run {run+1}: {len(subject_ids)} subjects with > {max_scrub} scrubbed volumes')\n",
    "    print(f'Subject IDs: {subject_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
